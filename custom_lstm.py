# -*- coding: utf-8 -*-
"""Custom_LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q2_6HajfbhUKE0UedlqtjCAbMB97HB--
"""

!apt install tesseract-ocr
!pip install pytesseract Pillow
!pip install PyMuPDF
!pip install scikit-learn

import os
import fitz
import pytesseract
from PIL import Image

def extract_text_from_pdfs(pdf_folder, output_folder="extracted_texts"):
    os.makedirs(output_folder, exist_ok=True)

    for filename in os.listdir(pdf_folder):
        if filename.endswith(".pdf"):
            filepath = os.path.join(pdf_folder, filename)
            doc = fitz.open(filepath)
            text = ""
            for page in doc:

                page_text = page.get_text()

                if page_text.strip():
                    text += page_text + "\n"
                else:

                    pix = page.get_pixmap(dpi=300)
                    img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
                    ocr_text = pytesseract.image_to_string(img)
                    text += ocr_text + "\n"

            doc.close()

            out_path = os.path.join(output_folder, os.path.splitext(filename)[0] + ".txt")
            with open(out_path, "w", encoding="utf-8") as f:
                f.write(text)

            print(f"Saved: {out_path}")


def load_texts_from_folder(folder_path):
    texts = []
    filenames = sorted(os.listdir(folder_path))
    for filename in filenames:
        if filename.endswith(".txt"):
            with open(os.path.join(folder_path, filename), "r", encoding="utf-8") as f:
                content = f.read().strip()
                if content:
                    texts.append(content)
    return texts
extract_text_from_pdfs("/content/drive/My Drive/pdf_folder")
all_texts = load_texts_from_folder("extracted_texts")



from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
tokenizer.save_pretrained("./tokenizer")

vocab_size = tokenizer.vocab_size

print(vocab_size)

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
from sklearn.model_selection import train_test_split
import torch.nn.functional as F
import random


tokenizer = AutoTokenizer.from_pretrained("./tokenizer")
vocab_size = tokenizer.vocab_size
pad_token_id = tokenizer.pad_token_id


def split_into_chunks(text, max_tokens=500):
    sentences = text.split('. ')
    chunks = []
    current_chunk = ""
    current_len = 0
    for sentence in sentences:
        sent_len = len(tokenizer.encode(sentence, add_special_tokens=False))
        if current_len + sent_len <= max_tokens:
            current_chunk += sentence + ". "
            current_len += sent_len
        else:
            chunks.append(current_chunk.strip())
            current_chunk = sentence + ". "
            current_len = sent_len
    if current_chunk:
        chunks.append(current_chunk.strip())
    return chunks


# making chunks pdf wise
all_docs_chunks = []  # List of lists

for text in all_texts:
    chunks = split_into_chunks(text,500)  # chunk one document
    all_docs_chunks.append(chunks)  # keep chunks grouped per doc

## Creating token ids
all_docs_token_ids = []

for doc_chunks in all_docs_chunks:
    doc_token_ids = []
    for chunk in doc_chunks:
        tokens = tokenizer.encode(chunk,
                                  add_special_tokens=True,
                                  max_length=512,
                                  padding='max_length',
                                  truncation=True)
        doc_token_ids.append(tokens)
    all_docs_token_ids.append(doc_token_ids)

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import sent_tokenize
import numpy as np
import nltk

nltk.download('punkt')

def generate_summary_tfidf(chunk, n):
    sentences = sent_tokenize(chunk)
    if n >= len(sentences):
        return sentences

    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(sentences)
    sentence_scores = tfidf_matrix.sum(axis=1).A1

    top_n_idx = np.argsort(sentence_scores)[-n:]
    top_n_idx_sorted = sorted(top_n_idx)

    summary = [sentences[i] for i in top_n_idx_sorted]
    return summary

from nltk.tokenize import sent_tokenize

def generate_summary_first_n_sentences(chunk, n):
    sentences = sent_tokenize(chunk)
    return sentences[:n]

## extractive summaries



def generate_training_data_grouped(all_docs_chunks, method="tfidf", n=2):
    training_data_grouped = []  # List of lists

    for doc_chunks in all_docs_chunks:
        doc_data = []
        for chunk in doc_chunks:
            if method == "tfidf":
                summary = generate_summary_tfidf(chunk, n)
            elif method == "first":
                summary = generate_summary_first_n_sentences(chunk, n)
            else:
                raise ValueError("Invalid method. Use 'tfidf' or 'first'.")
            doc_data.append((chunk, summary))
        training_data_grouped.append(doc_data)

    return training_data_grouped

# Creating token ids for summaries
import nltk
nltk.download('punkt_tab')

summary_token_ids=[]
training_data_grouped=generate_training_data_grouped(all_docs_chunks, method="tfidf", n=2)


for pdf in training_data_grouped :
  single_summary_token_ids=[]
  for chunk, summary in pdf:
    tokens = tokenizer.encode(" ".join(summary),
                          add_special_tokens=True,
                          max_length=512,
                          padding='max_length',
                          truncation=True)

    single_summary_token_ids.append(tokens)
  summary_token_ids.append(single_summary_token_ids)

# making pair of (chunk_token_ids,summary_token ids) by pdf
training_tokens=[]

for pdf_chunk, pdf_summary in zip(all_docs_token_ids,summary_token_ids):
  pdf_level=[]
  for chunk,summary in zip(pdf_chunk,pdf_summary):
    pdf_level.append((chunk,summary))
  training_tokens.append(pdf_level)

# Making a custom dataset for token prediction training

from torch.utils.data import Dataset
import torch

class TokenPredictionDataset(Dataset):
    def __init__(self, all_token_ids, max_len=512):
        self.examples = []

        for doc_token_ids in all_token_ids:  # List of docs, each with list of chunks
            for token_ids in doc_token_ids:
                # Truncate/pad
                token_ids = token_ids[:max_len]
                token_ids += [0] * (max_len - len(token_ids))

                input_ids = token_ids[:-1]
                labels = token_ids[1:]

                self.examples.append({
                    "input_ids": torch.tensor(input_ids, dtype=torch.long),
                    "labels": torch.tensor(labels, dtype=torch.long)
                })

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        return self.examples[idx]

"""## Model"""

import torch
import torch.nn as nn

class LSTMEncoderDecoder(nn.Module):
    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, pad_token_id=0):
        super(LSTMEncoderDecoder, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_token_id)  # Encoder embedding
        self.decoder_embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_token_id)  # Decoder embedding

        self.encoder_lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True,dropout=0.3,num_layers=2)
        self.decoder_lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True,dropout=0.3)

        self.fc_out = nn.Linear(hidden_dim, vocab_size)

    def forward(self, input_ids, target_ids=None):


        #  Encoder
        input_embeds = self.embedding(input_ids)
        encoder_output, (hidden, cell) = self.encoder_lstm(input_embeds)   # the _, is basically to say ignore all outputs except  hidden and cell state

        #  Token Prediction Mode
        if target_ids is None:
            # Predict next token from input
            outputs = self.fc_out(encoder_output)  # Shape: (batch_size, seq_len, vocab_size)
            return outputs

        #  Summarization Mode
        target_embeds = self.decoder_embedding(target_ids)
        decoder_output, _ = self.decoder_lstm(target_embeds, (hidden, cell))
        logits = self.fc_out(decoder_output)
        return logits

"""## Tokenization Training Loop"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import StepLR
from sklearn.model_selection import train_test_split


model = LSTMEncoderDecoder(embed_dim=128, hidden_dim=256, vocab_size=vocab_size)





train_data, val_data = train_test_split(all_docs_token_ids, test_size=0.2, random_state=42)
train_dataset = TokenPredictionDataset(train_data, max_len=512)
val_dataset = TokenPredictionDataset(val_data, max_len=512)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # cpu or gpu used
print("Using device:", device)




train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore padding token id 0

model.to(device)
model.train()

num_epochs = 10
epoch_losses = []  # to store loss at end of each epoch

best_val_loss = float('inf')
patience = 3
counter = 0


for epoch in range(num_epochs):
    total_loss = 0
    for batch in train_loader:
        input_ids = batch["input_ids"].to(device)
        labels = batch["labels"].to(device)

        optimizer.zero_grad()

        outputs = model(input_ids)


        outputs = outputs.view(-1, outputs.size(-1))
        labels = labels.view(-1)

        loss = criterion(outputs, labels)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()



        total_loss += loss.item()

    model.eval()
    val_loss = 0
    with torch.no_grad():
      for batch in val_loader:
        input_ids = batch["input_ids"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(input_ids)
        outputs = outputs.view(-1, outputs.size(-1))
        labels = labels.view(-1)

        loss = criterion(outputs, labels)
        val_loss += loss.item()

    avg_val_loss = val_loss / len(val_loader)
    avg_loss = total_loss / len(train_loader)
    epoch_losses.append(avg_loss)
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}")

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        counter = 0
        # Save best model
        torch.save(model.state_dict(), "best_model.pth")
        print("Best Model Saved")
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered.")
            break
    model.train()

#  Epoch 10/10, Loss: 5.5115, Val Loss: 6.2762
#  Epoch 10/10, Loss: 5.4041, Val Loss: 6.1774
#  Epoch 10/10, Loss: 6.6101, Val Loss: 6.9772 [Current]

torch.save(model.state_dict(), '/content/drive/MyDrive/model.pth')

import matplotlib.pyplot as plt

plt.plot(epoch_losses)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss Over Epochs")
plt.grid(True)
plt.show()

